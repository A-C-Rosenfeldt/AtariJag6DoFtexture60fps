=== new: Floats are better than lazy precision ===

So basically lazy precision is slow on JRISC like on all other real hardware.
I could easily tweak transistors to make this faster. Latency on the used DRAM would even be low enough to pull more precision throught the graph.
But there is a lazy light way of doing things:

Use floats with 16 bit mantissa ( not normalized because the IMUL instruction does not know that concept, and probably with sign in mantissa because XOR is so slow on JRISC).
32 bit vertices
subtract camera position
16 bit + exp floats
rotate
MAC is not really that fast, but still: A vertex better has a common expontent
I would almost have used normalized device coordinates, but the rounding into pixels killa the beam tree. Ah, no it does not.
So NDC is used to clip to the viewing frustum. JRISC is quite suited for this. Only thing is: the vertices of an edge might have a different expontent ( more than +-1).
	=> fall back to 32 bit . This is not really pure: I cannot justify to pull the 32bit through the rotation matrix
After that coordinates are mulitplied (and shifted) to mean pixel.subpixel  (9.7 bit or even 8.8 ).
Then the beam tree is constructed. It is really not a beam tree from the point of rounding, though. It is 2d, and is limited to the viewing frustum.
It has a z component though. So kind height field.
For fast maths on JRISC, z needs to also be 16 bit int. Our floats cover a larger range.
So for this ( even without a z-buffer), we need to define a near and far clipping plane. Or we keep the 32 bit fall back?
Z resolution happens on BSP merge. It is similar to scanline rendering, when on the leading edge we sort by z ( similar to sprites ).
This is quite expensive and may be the reason why the beam tree beats scanline rendering on Jaguar.
Now when we resolve polygons where all vertices are with the same expontent +-1 range ( I should allow only two expontents to not make the whole float concept ridiculous ),
we can use 16 bit math. Else we need 32 bit. Of course it is easy when we can just use the expontents to sort stuff out.
Actually, the problems start when one polygon has vertices with far different expontent. Like a floor or corridor walls. This infects all comparisons.
I am afraid that I cannot squeeze 16 and 32 bit math into 4 kB.
I also plan to use bounding boxes. I accept that I need to pre-calcuate "growing" vectors for the vertices to account for rounding.
Perhaps there is a way to shrinkwrap without creating pointy vertices. Anyways: I need this code to also fit into 4 kB.
Bounding boxes allow me to skip view frustum culling for complex meshes ( simpler code path, self modify code? ).
It allows me to skip floating within the box. It allows occlusion culling in the beam tree.
Also I want portal code at my finger tips. I wonder if there is a way to validate portals in 3d? We have foam, and must prove that only the portals allow beams to pass from one cell to the other.
So, a near and far clipping plane is a way to reduce code size.

Perhaps I could own the floating z? I split polygons along const-z ( like near and far plane ).
Just there would be no funny equation ( though, it is really funny: it is just 1/z minus some offset for the far plane. Then the near plane results from this).
1/z with rounding looks much more efficient for floats than for ints.
The weird thing with 1/z is, as we move backwards, the comparison becomes less precise on a given mesh. A stable mesh then starts to flicker.
z is of course impossible on 1993 era hardware. LoD does not really work on low poly 3d in 1993. So float z is better anyway. 
There could still be a happy path for expontent range max 1.

How expensive is this clipping? It only deals with z. The plane is not 0, but 2^n is easy enough.
It is the same expression as for the other clipping planes even after specalization.
I mean, the expressions originally use vectors, but we already know one ordinate clipping result.
Or is it really vector? With planes orthogonal to a main axis ( or easy 32 bit transform using ADD ), there is not much vector math left.
We use one ordinate to get the part we need to move along the edge. Then we blend the others.

I would be ugly if this would disturb the edges of the polygon as inserted into the BSP.
So this is like merging two BSPs.
The edge equations become independent of the vertices .. are cutting points really vertices?
So cutting points only have the integer part of the screen coordinates.
The calcuation for cutting takes in the 16 bit edges, and then does a division -> 8 bit.
Thanks to the high precision division unit on JRISC ( and SH2 for ports ), this is no problem.
Uh, how does this equation look again? View frustum culling had one aligned edge and already maxed out our precision.
Here we are in 2d, but does it help? The solution breaks the symmetry, not only between point 0 and 1 (as for the frustum), but also between x and y.

There may be an error in the beam tree code.
It seem like the temporary variables (s,t  -- originally meaning time or trajactory) are the easy way.

given: [s,t] * Matrix_2d = delta_cross
=>      [s,t] = delta_cross * inv(Matrix_2d) ( 16 bit / 32 bit)
then [s,t] * delta_inner = [x,y]
So it is 32 bit / 32 bit . And funny enough, for the integer cut-points this is also integer division.

So float z and beam tree are compatible.

What about the precision of the const z? So z wobbles around with the position of these cut lines?
2d interpoltation is similar to texture, but how do we deal with 4 vertices?
The direction values for all const-z lines need to be the same. Gradient is rotated. There may be a small jump at z lines.
This does not work. For textures I have accepted that I need safety distance in the atlas.
Jaguar hardware does not know clamp or saturate. Software can clamp per span on rectangular boundaries to avoid extreme color mistakes. Would not be so bad if CLUT was available to the blitter.
Flickering texturs are kinda cute, gaps in geometry are not.
Z rounding is okay. I guess with float I have enough leeway to accept some z fighting.
So take the direction (of the first cut I do), and scale it.
Make sure to overexpand the 16 bit range by delta.
For grazing incidence, the lines may become visible due to extreme expansion?
Grazing incidence looks weird anyway.



----- old Event based method ----

1000 vertices max per scene, so that I can store a whole dependency graph in memory.
All the calculations outweight the memory access penalty slightly ( on JRISC ).
Maybe I should create this graph only per frame. No need to pull the rest of the dungeon into this.
Floating point is a bit more costly than fixed point. MAC does not support floats.
But some calculations are so long that I don't know if I use the Multiplier to its full extend ( 15x15 because no carry or all data goes to Jerry ).
Floats can also employ variable precision and I have a strict error range in my adds and muls.
Fraction comes as last step always ( sometimes I only grab the sign of nominator and denominator).
Fraction error range can be checked by multiplication of the result.
So for example we have U=3.487 , then we multiply 2 * denominator and another value with two more denominator and check if those are outside of the nominator range. Then we know the resul: 3 .
Ah, this feels so weird the moment I think about texture interpolation or edge antialias.
With a beam tree we know the slope of our edges. So we should use our compute power for this instead of mislead pixel perfection.
The border pixels of a span in JRSIC need to be calculated by JRISC for aligned access anyway.
It hurts a bit that I would need to pull the information about the neighbour.
I may need to render a whole tree into a pixel with lots of crossings.
32 bit complete solves "scrolling" through a dungeon. I can live with EyeOfTheBeholder like dungeons at the start. Level can be 256 times larger than viewing distance.
Realistically, 32 bit fixed point with antialias and interpolation will look best. Antialias needs a smaller buffer, and is isotropic, in contrast to anisotropic pixels ( which the Jaguar can do).
Smaller framebuffer is need because the blitter has a low fill rate. 
Maybe it is as fast to edge antialias textures? Or range covers two or four texels, then blend.
So when precision of 16bit gets to its limit, the texture does not (only) jump, but blurrs.
Of course, it may make sense to base the blurr on the projection. This would work for small affine triangles. This again is an argument to feed texture projection through the vertices. Two code paths?
When I split up a large polygon for correction, the fragments are still affine and done by the blitter. So they have one blurr. So the blitter does the non-blur 16x16 bi-linear blocks, and JRISC does the edge filling triangles?
That sure will look ugly. I'd say, blurr fades out from 1:1 zoom to 1:2 zoom. Ah, feels like, I just have to try this. I don't want to show my edge phrase trick.
The blitter interpolates without perfect precision and without blurr. I guess, I should care less. Why waste perfect endpoints on a slope calculation which is rounded to fixed point?
Texture mapping runs in pixel mode. We rather want it to run long spans. Edge phrased by JRISC are just too slow. JRISC is only for the top and bottom corner, where the set-up time ob the blitter would be as bad. So we really have vertex-phrases.
Large polygons need to interleave a lot of calculations into their 16x16 bi-linear blocks. The edge triangles and the vertex phrases their vertices. 
Zoommed in textures need fill rate, and we let the blitter do that, while we concentrate on the edges. So in SRAM a buffer of edge phrases to fire, once the blitter has done another bulk renderer.


Explanation on the web needs typescript like the CPU-simulator.
Fun demos can run on the Jag which has limited precision for factors. So we can show off lazy added precision.
Similarly, 8 bit CPUs could try to calculate large runs on 8 bit.
Both CPUs have relative fast branches and memory access.

After trying to understand the 3d pipeline, I could not find a way to make following stuff work:
1) On a texture not leak outside the source region.
2) clipping so that the order of edges is naturally unchanged
3) consistent beam tree; even if I promote scanlines and pixel borders I still need 2) 
4) If I use a BSP to split and order polygons into fragments, I also need precision.
	Sometimes geometry is just so suited for this. Or I start with overlapping boxes, shrink, and half the faces end up on real geometry.
	Does anybody know why Doom has no fragments on inner nodes? I thought we pick polygons as splitters..

Graphics pipeline

Already without any tree the level is a mesh: Vertices, connected by edges, and faces which in turn have a texture.
Lightning on the target hardware can only be baked in or rather simple: No shadows, no use of precision math.
Edges are important for clipping against the viewport and in beam tree.
Date sits in a large flow graph and only local dependend stuff is pushed through it. No large vertex buffer!



Camera has 32 bit position and a transformation matrix with 16 bit entries ( allows typical field of views, but maybe no telescope):
The transition to parallel perspective needs to be implemented differently ( next iteration ).
To reduce the number of exceptions, I may want to use floating point with one exponent per vector. Components have no norm. Only 4 bit exponent.

Clipping per face of frustum. Near plane is at 0 : Don't render planes which cut the camera. 32 bit precision of position
in a large level still may hit this. Also people expect this in the code.
A blanket far plane makes no sense on the target platforms because they are so bad at skyboxes that we can rather use real geometry.

Field of view ( z-buffer scale) and screen aspect ratio (in pixels) make it so that the frustum planes are defined by a vector
with number of pixels * 2^n ( floating point: we always shift around bits. Only real division is expensive).
So clipping a vetex is not a simple comparison, but one side is multiplied by 3 (4) ( oldschool target platform has 3:4 AR).
So ah, 4 is 2^n. 3 is also onyl two bits. Now z scale is more free for me because z-buffer has only limited use on the target platforms.

A cut throuhg an edge results in a fraction ( nominator / denominator ). For perspecitve we divide by z and then one co-ordinate is exactly on the border of the screen,
but both 1/z and other/z have it and there is a sum somwhere and it stays.
Vertices are sorted by Y. In fact we multiply the denominator with the line number. Costs 8 bits. Will mostly suffice.
For triangel setup we need to know the slope of the edge: Another division.
So if the slope is not conclusive, we can always test 2 pixel instead of just one.
Also the target platforms like fixed points / floats. So we have a slope with a fraction and only check a pixel if we come close to it.
Similarly, for the slope of the texture, I can trace a ray to check which of the two or four texels I hit.

Now for a beam tree we need to cross edges to get a point. We even need to do it with 3 edges to decide if one edge needs to be split.
So, lots of divisions and multiplications. One could say that a beam tree really needs floating point.
It was discussed when Quake was new. Quake uses the floats. But it also seems that variable precision is most badly needed here.

To pull in more precision in a chain of MAC, the graph is followed backwards ( each edge has a direction but the back direction is also stored in RAM )
Each node can append more digits for the fraction. We need to know the error of our floats after division. Like:
slope of edge, slope of texture coordinates. We divide U and V by z at the vertex and calculate a slope.
Going along the slope we add those values up, but that is equivalent to multiplication.
We cannot add precision to this code path ( with floats and += ). Exception goes over to MAC and final division.
I don't see no easier way to calculate the uncertainty than by variing the nominator and denominator into their extreme values.
( +1 , -1  or 0, +1 ).

So texture mapping does not interfere with the beam tree, but I want to use at atlas and never have "general protection fault".
That Jaguar blitter can do linear spans. So how do I know that the line hits the correct pixels?
I cannot really. I can only take start and end pixel and the 3 edges in texture space and check for safety distanc > rounding errors due to writing to hardware registers of blitter ( inner loop in C64 ).
Bilinear makes even more sense because I need to check distance in 2d anyway. So it also works for affine. This would all work with floats and some branches .. so beam tree is really expensive? In terms of code size?

About that perspective correction: We divide per point ( span start, or upper left corner of block). So we can keep nominator and denominator up to that point. We can pull in more precision for both.
Only the subspan is does rounding. We say that it is hybrid between pixelation and real vector. We create no pixel gaps. First and last px are precise anyway ( integer ), only the gap is rounded.
When we round in UV space, we only need the precsion there. Grazing incidence on screen is no problem. Only slithers in an atlas woudl be a problem.

High precision clipping ( at screen border or occluders ) gives us high precsion U,V -- read UV within source domain.

Fractions for slope avoid edge crossing on rasterization just as with bresenham. So there is no new branch. Indeed with fractions, the exception branch is only taken seldomly and would perform on branchprediction hardware
like ARM or pentium.. .

BSP merging is a complicate algorithm which would generate a datadependency graph -- not use a given one. An exception in the clipping in the texture may very well reach through the whole Beam tree to up the precision.
Do I persist the whole graph with all pointers into (local) RAM, or do I regenerate it only in case?
The latter is the spirit of the exception. The code path needs to know where to concentrate on.
We have a tree .. which is most of our datadependency graph .. it even includes top level screen borders. 
No matter what the heuristics looked at, we just go up to the parents and get the clipping edges.
Each node could contain a sorted list of vertices of its convex shape2D. Edge definiton may be independent of vertices if that dependency chain is shorter that way.
Sorting information may only be acurate relative to scanlines. Or not even available? What if that subTree is hidden anyway?

Bounding volumes are meshes like everything else. So the beamtree really splits stuff up. Heuristics may want to use horizontal lines for a frame. Now I feel, like MVP should not already do this.

Usually shaders are expensive to load, but either way: Draw convex leaf of BeamTree or draw polygon with BSP as clip mask / scissor mask / stencel mask ( latter ones are usually stored as bitmaps).

Caching of transformed vertices is possible ( read: We don't need to have a transformed buffer as big as the original buffer ) because we use a bounding mesh hierarchy.
Those meshes can overlap and edges can reach into vertices in the next node. So the "current node" where we cache  is really a tree growing in the global tree.
Also this tree as links to from the beam tree. Already without the precision thing we would cache those.


It seems that lazy precision ( precision on demand ) needs BSP for 3d and bottom up beam tree. All vertices and documented beam tree decisions need to exists in memory in order to pull in more precision.
Also the camera position is pulled in lazily. So small movements don't trigger much changes .. if two BSP subtrees are close to each other and the camera far away.
The beam tree gives us convex polygons as leafs. Edges and vertices can have their own data. Because we don't round, they match. It just makes the nominator and denominator smaller.
Clipping is applied like in floating point .. only that the interface is now implemented by the infinite precision fraction class ( still wonder if I may need infinite precision floats ).

While the beam tree is constructed bottom up, of course child nodes are detected top down when their bonding box leaks. Likewise view frustum culling exposes new nodes. Invisible nodes are garbage.
I may need to look up this cache agnostic algorithms, but basically if the garbage is not in the way of defragmentation, I want to keep it as long as possible. The target systems are single threaded 
(okay, the Jag has two CPUs .. but assume that we have music) . Now a reappearing object may need deep inspection and a lot of repairs .. maybe decide based on relative camera movement.

So I don't see much way to back paddel to single frame images. Even the camera rotation, projection on scanlines, and Bresenham may want to pull in more digits.
I could back grow some tricks. Like when all leafs are sufficient convex, just round for the camera. But I don't see how this test is cheaper.

Infinite precision and/or floats need more cycles on JRISC. I estimate twice as much because JRSIC is already so cumbersome to use on integer math.
To make this viable, the precsion part must not dominate the rendering loop. Besically, I need to render simple scenes.
I guess, for edges I should stick with rounded values. 32/32 bit fractions should be good enough for anyone. In debug mode: Count the ambigious pixels.
Likewise texture mapping runs on rounded values. Those are based on floats. So the nominator and or denominator uses all availabe bits (32).
I use the address generator registers to point into a texture atlas. It accepts fraction values in different registers (interleaved x and y).
So I need some shifting after I fetch the fixed point result from DIV. I wonder if DIV is saturated. I may try to DIV the remainder to get the fractions,
but then again I onyl expect like 4 bits for the integer part of the delta registers and 6 or 8 bits for the integer base.

My examples: "Fight for Life" and 3d cars on an "Outrun" track don't need textures with GL_REPEAT. So small integer parts are okay.