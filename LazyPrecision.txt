1000 vertices max per scene, so that I can store a whole dependency graph in memory.
All the calculations outweight the memory access penalty slightly ( on JRISC ).
Maybe I should create this graph only per frame. No need to pull the rest of the dungeon into this.
Floating point is a bit more costly than fixed point. MAC does not support floats.
But some calculations are so long that I don't know if I use the Multiplier to its full extend ( 15x15 because no carry or all data goes to Jerry ).
Floats can also employ variable precision and I have a strict error range in my adds and muls.
Fraction comes as last step always ( sometimes I only grab the sign of nominator and denominator).
Fraction error range can be checked by multiplication of the result.
So for example we have U=3.487 , then we multiply 2 * denominator and another value with two more denominator and check if those are outside of the nominator range. Then we know the resul: 3 .
Ah, this feels so weird the moment I think about texture interpolation or edge antialias.
With a beam tree we know the slope of our edges. So we should use our compute power for this instead of mislead pixel perfection.
The border pixels of a span in JRSIC need to be calculated by JRISC for aligned access anyway.
It hurts a bit that I would need to pull the information about the neighbour.
I may need to render a whole tree into a pixel with lots of crossings.
32 bit complete solves "scrolling" through a dungeon. I can live with EyeOfTheBeholder like dungeons at the start. Level can be 256 times larger than viewing distance.
Realistically, 32 bit fixed point with antialias and interpolation will look best. Antialias needs a smaller buffer, and is isotropic, in contrast to anisotropic pixels ( which the Jaguar can do).
Smaller framebuffer is need because the blitter has a low fill rate. 
Maybe it is as fast to edge antialias textures? Or range covers two or four texels, then blend.
So when precision of 16bit gets to its limit, the texture does not (only) jump, but blurrs.
Of course, it may make sense to base the blurr on the projection. This would work for small affine triangles. This again is an argument to feed texture projection through the vertices. Two code paths?
When I split up a large polygon for correction, the fragments are still affine and done by the blitter. So they have one blurr. So the blitter does the non-blur 16x16 bi-linear blocks, and JRISC does the edge filling triangles?
That sure will look ugly. I'd say, blurr fades out from 1:1 zoom to 1:2 zoom. Ah, feels like, I just have to try this. I don't want to show my edge phrase trick.
The blitter interpolates without perfect precision and without blurr. I guess, I should care less. Why waste perfect endpoints on a slope calculation which is rounded to fixed point?
Texture mapping runs in pixel mode. We rather want it to run long spans. Edge phrased by JRISC are just too slow. JRISC is only for the top and bottom corner, where the set-up time ob the blitter would be as bad. So we really have vertex-phrases.
Large polygons need to interleave a lot of calculations into their 16x16 bi-linear blocks. The edge triangles and the vertex phrases their vertices. 
Zoommed in textures need fill rate, and we let the blitter do that, while we concentrate on the edges. So in SRAM a buffer of edge phrases to fire, once the blitter has done another bulk renderer.


Explanation on the web needs typescript like the CPU-simulator.
Fun demos can run on the Jag which has limited precision for factors. So we can show off lazy added precision.
Similarly, 8 bit CPUs could try to calculate large runs on 8 bit.
Both CPUs have relative fast branches and memory access.

After trying to understand the 3d pipeline, I could not find a way to make following stuff work:
1) On a texture not leak outside the source region.
2) clipping so that the order of edges is naturally unchanged
3) consistent beam tree; even if I promote scanlines and pixel borders I still need 2) 
4) If I use a BSP to split and order polygons into fragments, I also need precision.
	Sometimes geometry is just so suited for this. Or I start with overlapping boxes, shrink, and half the faces end up on real geometry.
	Does anybody know why Doom has no fragments on inner nodes? I thought we pick polygons as splitters..

Graphics pipeline

Already without any tree the level is a mesh: Vertices, connected by edges, and faces which in turn have a texture.
Lightning on the target hardware can only be baked in or rather simple: No shadows, no use of precision math.
Edges are important for clipping against the viewport and in beam tree.
Date sits in a large flow graph and only local dependend stuff is pushed through it. No large vertex buffer!



Camera has 32 bit position and a transformation matrix with 16 bit entries ( allows typical field of views, but maybe no telescope):
The transition to parallel perspective needs to be implemented differently ( next iteration ).
To reduce the number of exceptions, I may want to use floating point with one exponent per vector. Components have no norm. Only 4 bit exponent.

Clipping per face of frustum. Near plane is at 0 : Don't render planes which cut the camera. 32 bit precision of position
in a large level still may hit this. Also people expect this in the code.
A blanket far plane makes no sense on the target platforms because they are so bad at skyboxes that we can rather use real geometry.

Field of view ( z-buffer scale) and screen aspect ratio (in pixels) make it so that the frustum planes are defined by a vector
with number of pixels * 2^n ( floating point: we always shift around bits. Only real division is expensive).
So clipping a vetex is not a simple comparison, but one side is multiplied by 3 (4) ( oldschool target platform has 3:4 AR).
So ah, 4 is 2^n. 3 is also onyl two bits. Now z scale is more free for me because z-buffer has only limited use on the target platforms.

A cut throuhg an edge results in a fraction ( nominator / denominator ). For perspecitve we divide by z and then one co-ordinate is exactly on the border of the screen,
but both 1/z and other/z have it and there is a sum somwhere and it stays.
Vertices are sorted by Y. In fact we multiply the denominator with the line number. Costs 8 bits. Will mostly suffice.
For triangel setup we need to know the slope of the edge: Another division.
So if the slope is not conclusive, we can always test 2 pixel instead of just one.
Also the target platforms like fixed points / floats. So we have a slope with a fraction and only check a pixel if we come close to it.
Similarly, for the slope of the texture, I can trace a ray to check which of the two or four texels I hit.

Now for a beam tree we need to cross edges to get a point. We even need to do it with 3 edges to decide if one edge needs to be split.
So, lots of divisions and multiplications. One could say that a beam tree really needs floating point.
It was discussed when Quake was new. Quake uses the floats. But it also seems that variable precision is most badly needed here.

To pull in more precision in a chain of MAC, the graph is followed backwards ( each edge has a direction but the back direction is also stored in RAM )
Each node can append more digits for the fraction. We need to know the error of our floats after division. Like:
slope of edge, slope of texture coordinates. We divide U and V by z at the vertex and calculate a slope.
Going along the slope we add those values up, but that is equivalent to multiplication.
We cannot add precision to this code path ( with floats and += ). Exception goes over to MAC and final division.
I don't see no easier way to calculate the uncertainty than by variing the nominator and denominator into their extreme values.
( +1 , -1  or 0, +1 ).

So texture mapping does not interfere with the beam tree, but I want to use at atlas and never have "general protection fault".
That Jaguar blitter can do linear spans. So how do I know that the line hits the correct pixels?
I cannot really. I can only take start and end pixel and the 3 edges in texture space and check for safety distanc > rounding errors due to writing to hardware registers of blitter ( inner loop in C64 ).
Bilinear makes even more sense because I need to check distance in 2d anyway. So it also works for affine. This would all work with floats and some branches .. so beam tree is really expensive? In terms of code size?

About that perspective correction: We divide per point ( span start, or upper left corner of block). So we can keep nominator and denominator up to that point. We can pull in more precision for both.
Only the subspan is does rounding. We say that it is hybrid between pixelation and real vector. We create no pixel gaps. First and last px are precise anyway ( integer ), only the gap is rounded.
When we round in UV space, we only need the precsion there. Grazing incidence on screen is no problem. Only slithers in an atlas woudl be a problem.

High precision clipping ( at screen border or occluders ) gives us high precsion U,V -- read UV within source domain.

Fractions for slope avoid edge crossing on rasterization just as with bresenham. So there is no new branch. Indeed with fractions, the exception branch is only taken seldomly and would perform on branchprediction hardware
like ARM or pentium.. .

BSP merging is a complicate algorithm which would generate a datadependency graph -- not use a given one. An exception in the clipping in the texture may very well reach through the whole Beam tree to up the precision.
Do I persist the whole graph with all pointers into (local) RAM, or do I regenerate it only in case?
The latter is the spirit of the exception. The code path needs to know where to concentrate on.
We have a tree .. which is most of our datadependency graph .. it even includes top level screen borders. 
No matter what the heuristics looked at, we just go up to the parents and get the clipping edges.
Each node could contain a sorted list of vertices of its convex shape2D. Edge definiton may be independent of vertices if that dependency chain is shorter that way.
Sorting information may only be acurate relative to scanlines. Or not even available? What if that subTree is hidden anyway?

Bounding volumes are meshes like everything else. So the beamtree really splits stuff up. Heuristics may want to use horizontal lines for a frame. Now I feel, like MVP should not already do this.

Usually shaders are expensive to load, but either way: Draw convex leaf of BeamTree or draw polygon with BSP as clip mask / scissor mask / stencel mask ( latter ones are usually stored as bitmaps).

Caching of transformed vertices is possible ( read: We don't need to have a transformed buffer as big as the original buffer ) because we use a bounding mesh hierarchy.
Those meshes can overlap and edges can reach into vertices in the next node. So the "current node" where we cache  is really a tree growing in the global tree.
Also this tree as links to from the beam tree. Already without the precision thing we would cache those.


It seems that lazy precision ( precision on demand ) needs BSP for 3d and bottom up beam tree. All vertices and documented beam tree decisions need to exists in memory in order to pull in more precision.
Also the camera position is pulled in lazily. So small movements don't trigger much changes .. if two BSP subtrees are close to each other and the camera far away.
The beam tree gives us convex polygons as leafs. Edges and vertices can have their own data. Because we don't round, they match. It just makes the nominator and denominator smaller.
Clipping is applied like in floating point .. only that the interface is now implemented by the infinite precision fraction class ( still wonder if I may need infinite precision floats ).

While the beam tree is constructed bottom up, of course child nodes are detected top down when their bonding box leaks. Likewise view frustum culling exposes new nodes. Invisible nodes are garbage.
I may need to look up this cache agnostic algorithms, but basically if the garbage is not in the way of defragmentation, I want to keep it as long as possible. The target systems are single threaded 
(okay, the Jag has two CPUs .. but assume that we have music) . Now a reappearing object may need deep inspection and a lot of repairs .. maybe decide based on relative camera movement.

So I don't see much way to back paddel to single frame images. Even the camera rotation, projection on scanlines, and Bresenham may want to pull in more digits.
I could back grow some tricks. Like when all leafs are sufficient convex, just round for the camera. But I don't see how this test is cheaper.