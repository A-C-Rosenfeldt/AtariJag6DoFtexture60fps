=== 2025-11-09 ===
Float and precision match ( see float.asm )
JRIC code does somehow stay compact.
precision can use the same exponent as all components. This leads to more natural code than a second exponent.
Precision is upgraded in 16 bit steps to match the storage. Some operations can only add 14 bits, so some storage will be wasted,
but not really. These operations start with 14 bit to even stay below 32 for the result.
Yeah, rounding degenerates the precision through the chain, yet I feel like it is difficult to decide for high precision -> precision funnel.
I hope that this comes automatically due to dependency. Important values will be backtracked to often. This would mean to swap out code. This would kill the code cache.
I can abort the current polygon. I can forward trace the vertices (just the pointers, no math) for cheap. Additional code sounds like a "second system" to me.
So, just abort the polygon and add allocate it as a new wider structure with 32bit fields (double links)
Then load backtracking code, mark with iterationCounter. The +=16 bits in mantissa rule will go all the way back to the rotation matrix.
Some traces will stop because they encounter a link to the next higher precision version of the vertex.
Run through all forward steps again, this time on the higher precision versions of the edges.
Tolerance is again calculated. Tolerance is not split into 14bit sections, but a single 32 bit value ( I only use add and shift, which are 32 bit on JRISC)
Tolerance can overflow starting with precision 3. Then: cut off the lower bits of the value. This can basically replace the fixed point math where 1.14 * 1.14 = 1.28 basically means throwing away 14 bits by default.
This is consistent with the rounding error added to tolerance. I just mean that we could perhaps only need one branch. On first pass, I should abort if tolerance has eaten up half of the bits?
These are still correction values, are they? But on the other hand, it would be faster now, to just throw away the old calculations at higher tolerance.
	(only the least significant 14 bit section)
	Ah, no, we need correction carry. This also shows that carry needs to have a sign. This shows that carry needs two bits. IMUL is due to vectors, so don't blame JRISC.
	Are 4 bit enough? I think that a correction only needs +-1. So it has the same range as the carry from inner product ( accidental coincidence ).
There are types of nodes. The number of input edges is constant per type.
So allocation is always like:  struct node{edges[]}  .
Set all the y start values.
Run the rasterizer ( the rasterizer always expects y_int different from the top vertex due to subbpixel precision.)
Oh, subbpixel and persepective correction now also want higher precision. Edge equations are now 16bitPixel * 32bit coeeficients.
So really: Load different code:  16bit pixel * (n*16 bit) coeeficients.
So actually, Bresenham can use Add on JRISC. So once we sit on an edge, we can
ADD 32,32   .
If on zero {   ; this is a jump because it is an exception, but this whole doc deals with exceptions, so here it is a block
	use IMAC to improve precision   ( no need to log code. This one just jumps into the latter part of subpixel precsion, which fits into our 4kB)
	;notice how we throw away this result for the next scanline: the low precision coeeficients and akkumulator are already optimal in the limits given by rounding.
	; -> no coupling, no bugs
}

16 * (n*16) needs a lot of shifts. Cannot really use MAC?

for n=m to 0 {  '0 is the probably state. m is arbitrary ( well <6 but you know)  . Step is implied in my language. If you want step, use C for loop
	load coeeficients   
	imultn x
	imac   y
	resmac s
	load 0, hi
	add lo,s
	adc 0,hi   ; no other way to carry a carry flat around without loop unrolling ( 4kB limit!! ). 0 eats a register because addq has no adc version.
	; To write more MIPS like, clean code without flags, mantissa strings need to adhere to 14 bits from the start. 15 bits. 
	;I invented 14 for the inner product. But do we need it? length is the product. lenght squared. Think of corner
	;Looks like I don't get around two register shift.

	;shifting load .  I mean, I need something like this already for 16 bit.
	subq 14, cx
	If carry {
		copy a,b
		addq SI
		load [SI],a
	}
	and 31,cx
	SAR 14,shifted    ; Cut off works .  Looks like keeping the shifted values saves us on instruction. Now: We need an idle pass throught the whole loop to init values.
	copy a,as
	SH cx,as   ; CX needs to be positive ( we are no yet "there" in a) and decreasing .  Cut off works
	ADD as,shifted   ; needed for sign
}


;inner nodes need to stream out the product. The shifter code just needs to be reversed. 
;somehow working with 32 bit is .
ROR 
AND low
store
AND high
store 

# For precision correction we need to store the full akkumulator
# to go full MIPS, we use a lot of carry in akku. Synergy between precision and cross product: upto 3 carry bits. Even 1 carray works: Sign means SBC. zero means nothing.
# do this sign thing on load and store 
# flag free version
shr 31, lo
add lo,hi  ; on store  .. what if carry happens here? we don't care, we actually load 14 bits?
sub lo,hi  ; on load  .. okay, feels weird to store signed if a shift anyways?

math
	int14

carry: int32
lo -> hi
rmw
	load 32 unsigned
	# JRISC cannot detect overflow. Here : int + unsigned is super sepcial anyway
	add mod, stored 
	 if  SHR 31,mod then sbc stored-1
	 else adc stored-1

To get rid of flags, I need to go through the complete stored akkumulator
from lo to hi   ;forced by carry on math, but also on load and store
	stream in akkumulator 14 bits at a time ( this happens on both operands, too )
	if previously streamed value <0 then --
	math
	local akku can be negative.
		SAR akku
		load akkuS
		SHL akkuS
		and 0011,akkuS  ; top bits are for carry . akkus is unsigned (stored as, no double cary please). So: stored signed akku would be beneficial
		Add akkuS,akku   ; akku s does not overflow as per the general arument about the top bits

	stream out the lowest 14 bit to buffer {*}. If negative then local akku++  ; round commercially to keep akku most precise . Storing directly and signed to 32 bit is only fast for the first run
	*local bit buffer     . For some reason, signs make no sense in long number. Could bits be +1/2 -1/2 ? IMUL does not work with these. Need to add bias? 1111 would be +1/2 and 000 -1/2. We lose the lsb.
	1/2 could be corrected
	MUL A-100,B-100 -> C   # bias.int    so we flip the sign? xor -1 ?
	C += (A+B )>>16  #bias.frac   theoretically, this should preserve the 32 bit  ADd to the fraction part of the result
		A+B should correct the 0*B and A*0 case . But it still is normal digits 10101. I need to add 1000,5 to go into +-+-+-+-+-
		So really 

	I feel like 1/2 just add extra steps
	1/2 * 1/2 = 1/4
	1000 * 1000 = 1000000
	0 * 0 = 0  ; 0 needs to be used for positive to keep the balance
	1 * 1 = 1


	if full
		store
		copy akku again
		shift

	akku string needs to be signed ( on 32 EG ), so that we pull commercially rounded values.
	There is no rounding cascade 01010   010.10 -> 011  01.1 -> 10   but   01.010 -> 01
	So, to be able to pull limited precision, the akku string needs to be made of unsigned words
	We round on read.
	Or we use fixed segments of 14 bits.

	I need to store the new akku for fresh users. I need delta for correction runs.

node{
	indices

	int32 akku
	# what happens when precision is added? Others pull on this. Do they expect signes? Yeah, on 32bit boundarais

	#dz

}


For variable precision the start is a bit akward because the of the two low precision bits. But really those still allow us to add the next 
MUL lo,lo
load 2bits from hihi
Add lo,2bits    ; so obviously I need to have stored the 2bit with 2bit headroom . Clearly this is needed for value alignment.
; the top two bits now contain the carry  . So at least half as akward as having only a single bit set.

copy 2bits, fromAbove
load hi
shr fromAbove, 30
add fromAbove, hi   ; this is not allowed to trigger  Exponent++, but thankfully, our compontents are not normalized, and the same 2 bits headroom we used before, still hold



;Variable precision multiplication walks over an L pattern if we only calcuate the change.
ForEach outputDigit n iterations
	for L_arm  2 iterations
		for input0_digt  1 iterations   the L is thin  => 2 carry bits it is. Really only one. but.  If I stick to 14 bits. 
			;Alignment would mean that every even 14 bit just go through, and every odd need a  
				cpy pre, factor  
				addq 1,SI
				test 1,SI				;alternative: toggle extra register. 
				if even 
					SHR factor
					load [si],pre   ;are low bits of si just ignored for local memory?
				maths
			. With more and more carry bits, and code size consideration, this starts to looks attractive . I still hate the double register shift. Now it is even aligned to 16. Thorn in my eye
			MUL    ; Simple MUL because clearly dynamic grouping of IMULTN IMAC RESMAC is slow . This is not hardware!!
			ADD      arms and carry_in
			copy 0
			ADC , carry_out   ; just find out how to do the sign. Correction factors are still signed. Well, they don't have to, but I would lose one bit precision
	16 bit output shift




;(variable shifter on both inputs very much looks like "smart idea")
;starting from lo,lo ( no carry space needed -- but consider synergy with sum in cross or inner) ln n / ln 2 bits   . Still looks like a wast of MUL to me

=== Floats are better than lazy precision on fixed ===

So basically lazy precision is slow on JRISC like on all other real hardware.
I could easily tweak transistors to make this faster. Latency on the used DRAM would even be low enough to pull more precision throught the graph.
But there is a lazy light way of doing things:

Use floats with 16 bit mantissa ( not normalized because the IMUL instruction does not know that concept, and probably with sign in mantissa because XOR is so slow on JRISC).
32 bit vertices
subtract camera position
16 bit + exp floats
rotate
MAC is not really that fast, but still: A vertex better has a common expontent
I would almost have used normalized device coordinates, but the rounding into pixels killa the beam tree. Ah, no it does not.
So NDC is used to clip to the viewing frustum. JRISC is quite suited for this. Only thing is: the vertices of an edge might have a different expontent ( more than +-1).
	=> fall back to 32 bit . This is not really pure: I cannot justify to pull the 32bit through the rotation matrix
After that coordinates are mulitplied (and shifted) to mean pixel.subpixel  (9.7 bit or even 8.8 ).
Then the beam tree is constructed. It is really not a beam tree from the point of rounding, though. It is 2d, and is limited to the viewing frustum.
It has a z component though. So kind height field.
For fast maths on JRISC, z needs to also be 16 bit int. Our floats cover a larger range.
So for this ( even without a z-buffer), we need to define a near and far clipping plane. Or we keep the 32 bit fall back?
Z resolution happens on BSP merge. It is similar to scanline rendering, when on the leading edge we sort by z ( similar to sprites ).
This is quite expensive and may be the reason why the beam tree beats scanline rendering on Jaguar.
Now when we resolve polygons where all vertices are with the same expontent +-1 range ( I should allow only two expontents to not make the whole float concept ridiculous ),
we can use 16 bit math. Else we need 32 bit. Of course it is easy when we can just use the expontents to sort stuff out.
Actually, the problems start when one polygon has vertices with far different expontent. Like a floor or corridor walls. This infects all comparisons.
I am afraid that I cannot squeeze 16 and 32 bit math into 4 kB.
I also plan to use bounding boxes. I accept that I need to pre-calcuate "growing" vectors for the vertices to account for rounding.
Perhaps there is a way to shrinkwrap without creating pointy vertices. Anyways: I need this code to also fit into 4 kB.
Bounding boxes allow me to skip view frustum culling for complex meshes ( simpler code path, self modify code? ).
It allows me to skip floating within the box. It allows occlusion culling in the beam tree.
Also I want portal code at my finger tips. I wonder if there is a way to validate portals in 3d? We have foam, and must prove that only the portals allow beams to pass from one cell to the other.
So, a near and far clipping plane is a way to reduce code size.

Perhaps I could own the floating z? I split polygons along const-z ( like near and far plane ).
Just there would be no funny equation ( though, it is really funny: it is just 1/z minus some offset for the far plane. Then the near plane results from this).
1/z with rounding looks much more efficient for floats than for ints.
The weird thing with 1/z is, as we move backwards, the comparison becomes less precise on a given mesh. A stable mesh then starts to flicker.
z is of course impossible on 1993 era hardware. LoD does not really work on low poly 3d in 1993. So float z is better anyway. 
There could still be a happy path for expontent range max 1.

How expensive is this clipping? It only deals with z. The plane is not 0, but 2^n is easy enough.
It is the same expression as for the other clipping planes even after specalization.
I mean, the expressions originally use vectors, but we already know one ordinate clipping result.
Or is it really vector? With planes orthogonal to a main axis ( or easy 32 bit transform using ADD ), there is not much vector math left.
We use one ordinate to get the part we need to move along the edge. Then we blend the others.

I would be ugly if this would disturb the edges of the polygon as inserted into the BSP.
So this is like merging two BSPs.
The edge equations become independent of the vertices .. are cutting points really vertices?
So cutting points only have the integer part of the screen coordinates.
The calcuation for cutting takes in the 16 bit edges, and then does a division -> 8 bit.
Thanks to the high precision division unit on JRISC ( and SH2 for ports ), this is no problem.
Uh, how does this equation look again? View frustum culling had one aligned edge and already maxed out our precision.
Here we are in 2d, but does it help? The solution breaks the symmetry, not only between point 0 and 1 (as for the frustum), but also between x and y.

There may be an error in the beam tree code.
It seem like the temporary variables (s,t  -- originally meaning time or trajactory) are the easy way.

given: [s,t] * Matrix_2d = delta_cross
=>      [s,t] = delta_cross * inv(Matrix_2d) ( 16 bit / 32 bit)
then [s,t] * delta_inner = [x,y]
So it is 32 bit / 32 bit . And funny enough, for the integer cut-points this is also integer division.

So float z and beam tree are compatible.

What about the precision of the const z? So z wobbles around with the position of these cut lines?
2d interpoltation is similar to texture, but how do we deal with 4 vertices?
The direction values for all const-z lines need to be the same. Gradient is rotated. There may be a small jump at z lines.
This does not work. For textures I have accepted that I need safety distance in the atlas.
Jaguar hardware does not know clamp or saturate. Software can clamp per span on rectangular boundaries to avoid extreme color mistakes. Would not be so bad if CLUT was available to the blitter.
Flickering texturs are kinda cute, gaps in geometry are not.
Z rounding is okay. I guess with float I have enough leeway to accept some z fighting.
So take the direction (of the first cut I do), and scale it.
Make sure to overexpand the 16 bit range by delta.
For grazing incidence, the lines may become visible due to extreme expansion?
Grazing incidence looks weird anyway.

Z-precision vs xy precision

It feels weird to calcuate z and its gradients from the rounded screen coordinates. Probably 16 bit for direction (even with wasted guard band) and independent 16 bit for distance are enough.
How far can you even view in descent? And do we appreciate the pixel mess far away in doom? Simulators with LoD surface may need a larger viewing distance.
The z-buffer in the Jaguar has 16 bit precision. It would be logical that the expensive buffer as a lower precision than what the GPU uses.
The blitter thinks so, and uses 16.16 internally for z. Same for the gradient. So I probably should also use this precision when I "jump" around using GPU mul.
One jump factor is the byte from the screen coordinate. So it is still quite fast on JRISC. And compoact. Or is it? 4k is not much.
I can only have so many passes. Probably, the small code scratchpad will be the main reason why T&L and blitting cannot happen in parallel.
Now I do not need to think about: Texture coordinates could run in parallel with blitter government, but, texture coordinates need their own pass??

The great thing about deformed meshes and UV maps in screen space is that I only need to invert a 2d Matrix. Multiplication of 2d matrices also is twice as fast as 3d.
Interpolation of floats can be done using const-z splits, but I want to postpone this to version 2.
Rather I want to use 32 bit integers. No random splits which complicate the debugging of the beam tree and wreck the aesthetics.
I could even convert float back to 32 bit int. At this poinst, I don't care anymore.
I feel like 32x16 math is the norm. I would only need one code path also for clipped vertices.
32 bit math can run over 64k steps before rounding becomes as severe as in 16 bit.

In screen space I already have kinda 32 bit: I want subbpixel precsion. So there is one MUL with the . Ah scrap that.
In screen space I have 8 x 32 bit for subbpixel precision ( for textures -- ah and z ).
Then I have another 8 x 32 bit multiplication to jump around pixels ( for the blitter and to compare z in a pixel in a convex sector).

With the guard band clipping co-ordinates become useless. I really want NDC and screen to be mappend by a shift operation so that I can absorb this into my float module.
NDC is a power of two number. So I cannot even use integer division to arrive at pixels -- other then the integer 2.
Since z and textures cannot be perfect anyway ( I know, I tried ), I can just as well mix ray tracing into this. So not only in screen corners I would trace for s,t and z,
but for all clipped polygons. Inverse rotation matrix is just the transpose, isn't it 
(I use square pixels (up to power of 2) and square NDC, squish would only be one reciproce .. or one row if multiplied in)?

So I already have it in memory.
I would skip the subbpixel calcuation (on the happy path for triangles within the guard.)
I really hope that this code also fits into the 4k local RAM. Perhaps it is a blessing that I cannot govern the blitter while doing this. The code for that looks quite fat.
The beam tree cares for DMZ in the square between the pixel dead-on rays ( even for forward rasterization, thanks to my precision, they are basically still there ).
I don't want to solve the rotation order of multiple cuts. So edges just end on one face of the DMZ (demilitariazed zone).
Of cource I need to make sure that edges don't cross outside. I mean, it would just trigger a new DMZ.
Looks like I need this DMZ also on the screen border, but only on pixel precision ultimately.

Ray tracing needs 3d Matrix inversion. Even if 3d is not as nice as 2d, we still can formulate this a little more friendly using cross-produc ( outer product ) and inner product.
Surface normals. s,t <-> u,v transformation is 2d. So inversion is transpose cong.. This hints at the rounding: It should not be problematic. Surely 32 bit will solve every problem.
Now I wonder If I could calcuate the (precise) gradient of a clipped edge from 3d vector maths?
Gradient is the normal vector of the plane going through the camera and the edge. The z-component is shifts the edge to hit the vertex on screen (or to create correct results for a passing edge like a horizon).
Yeah, so obviously this is just a cross product between the end points of the edge ( endpoints treated as vectors from the camera position).
For clipping I argued for 32 bit in the past. Here, both vectors would be needed to be treated as 32 bit. So a lot of MULs and perhaps a reason to accumulate.
In the end of course, I can float the whole vector down to 16 bit because the actual length of the vector is not important.
For compact code I probably need to loop over the components and store them with pointer++? Keep the max mantisse.
Then on pointer-- load, shift them.

How do I merge a pure edge ( no vertex on screen "horizon" ) in the beam tree? I could place it above. For the other tree I can easily decide per vertex if lies below or above the horizon.
The crossing between two edges is 16 bit math as with normal edges with their enpoints on screen. Solve for (x,y) ( 2d Matrix inversion).
I wonder if I should use two divisions here. I am on 16bit screen space which does really not like rounding.
I could use a rasterization trick: For one of the edges check into which sector the gradients falls.
Ah crap. So rather calcuate the silly 1/det  (integer?) multiply with the vector nominator, and check both results using MUL and CMP (code size! arg!! Probably div can be hidden)?

This trick is somewhat equivalent to dialing in to a 3x3 pixel -- 2x2 DMZ block. Then check if the center pixel is below or above the edges.
Then for each edge depending on the quadrant of its gradient check pixels on the "cross". Finally we know on which border the edge hits the DMZ.
If both edges disagree on the DMZ ( one of the four), do we detect this early? 
Edge-Edge comparison on the DMZ border seems to be rather tedious. Better to do it right from the start.
The thing is, we need to multiply the inverse gradient with the vertices. So we then deal with 32 bit. DIV for some funny reason accepts 32 bit on JRISC.
But when we try to check the last bit, we would need to to 32 bit MUL .. uh.

Anyways, we knoe the DMZ which containts the cut. Only thing which is left for us to do to allow BSP merge is to check on which of the four borders the edges enter.
This is 8x16 math and easy. Code size is so-so.

How do merge beam trees with DMZs. I feel like DMZs go first? Per level?
DMZ make sectors not convex anymore. Hmm. I can sort DMZ on the sides of a tree-edge. If the edge passes the DMZ, this no problem? I always had cuts on the edge.
Now the DMZs sit like beads on a string. Of course, both children need to take these "beads" into account. On merging, beads can fall togehter.
If a sector is filled by its beads, or already if a line is filled ( covered ), I see no reason to keep it alive.
This may be crucial for sane execution time of high detail, high important objects far away ( see typical scene from Iron Soldier or checkered flag  ).
Ah, the sector sit on the pixels. So they are never coverd by DMZ. Uh, a cut is not allowed to hit a pixel because then I don't know the color to draw. Remember the top-left rule!

In the MVP I want to use affine texture mapping because perspective correction really is a second step. As for z, I probably need 32 bit values, but as for z, pixels are <16 bit.
Then just apply subspan like Quake (+90° decider). When I raytrace the texture coordinates, there probably is some z in the denominator. For happy path: Multiply z back in? Sounds weird.
The happy path is essential for debuggin. So I start with affine texture mapping. But what do I do on clipping?
To keep the inner rasterizer, I need affine triangles. I know that perspecitve edges look better, but then I need to set-up the vertices in the happy path for this.
Clipping gives me all information for perspecitve. I feel like I should flat shade clipped polygons above a certain size until the beam tree works.
Then I go full subspan (Quake, Descent) rendering for everything clipped ( beam tree, portal, screen (not even NDC) ) and/or above a certain size / delta-z.
Basically, only small happy path triangles stay affine. Jaguar has no GL_REPEAT, so there will be many small triangles. I should invest in drawin pairs of them with a shared texture.

Raytraced pixels and DMZ are not screen space vertices. It is impossible to triangulate the polygon from this ( sort by z, zig-zag).
I kinda like z-zigzag because it could even work in PS1, but it really is not good for Jaguar.
For Jaguar rather it may be interesting for larger polygons to align the spans similar to the 8x8 bilinear tile method in terminal velocity.
Just with the blitter it makes no sense to have 1:1 aspect ratio. Set-up is so expensive that I can just as well do the full calcuation per scanline.
For this, try to merge all sectors of the polygon. I don't know why doom has trouble doing this in the column buffer.
But with a beam-tree, we collect all leafs which end up in this polygon. Then per scanline we know all the leafs to look at in a O(log(n)) fashion.
So the kinda artificial beam tree will not be visible like a ghost on the texture. Only anoyiance will be the 45° pops.
If Jaguar really can also do diagonal blitter runs, this would reduce the pops. Const-z slices would be anchored to the screen center and only rotate. Or polygon cg (mean value of (triangle) vertices)?
{\displaystyle C_{\mathrm {x} }={\frac {1}{6A}}\sum _{i=0}^{n-1}(x_{i}+x_{i+1})(x_{i}\ y_{i+1}-x_{i+1}\ y_{i}),}
{\displaystyle A={\frac {1}{2}}\sum _{i=0}^{n-1}(x_{i}\ y_{i+1}-x_{i+1}\ y_{i}).}
Different saching strategy for scale down and scale up. ( nearly 1:1 is faster without any cache). Scale up may be the only cacheable thing.
Is it possible to first load a const-z slice into color ram? No it is not becaue the blitter does not wrap around.
Still no argument for triangles. The real solution for scale up is to draw quads composed of 16x16 pixel from the CLUT (which I don't plan to use).
Similarly, since GL_REPEAT is missing, I want to draw quads for GL_REPEAT. This mesh of quads is constructed after the beam tree and everything finished.
I don't see any synergy here. I may want to use both kinds of quads on the same polygon: For a large floor for example.
It may make sense to extract a subtree only for this polygon. Then like normal pull texture-mapping project the vertices of this tree back onto the (repeated) texture.
Now we have a tree there and rasterize it to draw all visible quads. I love affine so much, but I guess, I should interpolate over the edges and then the spans here. Like Sky-hammer.
I should probalby also do that for clipped triangles. So I need to sub-pixel correct twice on every scanline. Clearly, not the happy path.
I would be a nice show to have options to let the tester try out affine vs quads. With fps gauge.

For the Rasterizer we still know the scanlines the polygon covers. But we need to check where the edge exits the DMZ. If any edge to the side then one more than if both on the bottom.
I need to deal with zero pixel spans anway. So could just use the edge equations and cover the full y-range.
If the span contans a DMZ linked to one of the edges, we need not calcuate the edge. But we need the accumulator on the next line.
So only drop calculation if and edges is covered with DMZ for the rest of the height .. Or don't because code gets too fat.

-- Lazy precision: another try --

Lazy precision should only be triggered very seldomly because it is so slow. So we should start with a the high precsion float / occasional 32 bit math.
Then when a vertex in the beamtree falls to close the border of a pixel, or a span falls to close to pixel border, or a texel or texel + delta falls to close to a texel border,
or a z comparison has a delta below, the precsion can be upped.

Expontent stays. Since I need to ADC any values any way and temporary storage is kind expensive, the whole calculation is repeated.
This clearly is an expansion on the original design. As an intermediate step I need counters for the above mentioned triggers.
With 8 bit subpixel precision, I can roughly estimate that every 100th calculation needs to be repeated? And probably no glitch was visible..
There is no synergy with the 32 bit clipping math. 32x32 math is kinda expensive, but not thaaat expensive like event based lazy precision.
16 bit clipping is not even expensive, so no need for a general guard band. Only reason would be to use shift between clipping coordinates and pixels. But then need to clip against beam tree a little more.
Is clipping even real 32 bit?
t = x_0 / x_delta    
y = y_0 + y_delta * t
z =
what if I do the last two as floats? Use the vertex close to the camera as base. Then direction to far away ( all 16 bit so far ). t as 16 bit is okay also.
Then the nasty float add strikes: Compare exponents 
Then we get a real 3d point clipped . Then divide ( don't really care for the double rounding)
We need z anyway.

If we care for rounding we could calculate y like this (closest vertex means all compontents!):
projected_y = (y_0 + y_delta * t) / (z_0 + z_delta * t )   
projected_y = (y_0*x_delta + y_delta * x_0) / (z_0*x_delta + z_delta * x_0 )

delta is calcuated in 32 bit ( even before camera shift? ). Can even calcuate the exponent before rasterization.
Then if delta is needed, it needs to be rotated like a vertex. Thankfully, no homogenous coordinates in use. I mean, I would not understand what they do here. 
So, these checks are kinda expensive and justify a guard band. A full matrix rotation, huh?
Float subtraction sounds cheaper. But thre are 5. The 3 deltas. Kinda as expensive as the bitscan comming from 32bit.
Then the partial move along the edge also leads to float adds. But no matter what: Both methods need this.
And then repeat this all for the UV texturer coordinates? Oh, I really need to interleave transformation with blitting. Both are slow.
But I need the whole beam tree before I can rasterize. Only texture coordinates can be late.
No lightning is very confusing. Flat shading really makes 3d pop. So no need for Gouraud auf low poly textures.

----- old Event based method ----

1000 vertices max per scene, so that I can store a whole dependency graph in memory.
All the calculations outweight the memory access penalty slightly ( on JRISC ).
Maybe I should create this graph only per frame. No need to pull the rest of the dungeon into this.
Floating point is a bit more costly than fixed point. MAC does not support floats.
But some calculations are so long that I don't know if I use the Multiplier to its full extend ( 15x15 because no carry or all data goes to Jerry ).
Floats can also employ variable precision and I have a strict error range in my adds and muls.
Fraction comes as last step always ( sometimes I only grab the sign of nominator and denominator).
Fraction error range can be checked by multiplication of the result.
So for example we have U=3.487 , then we multiply 2 * denominator and another value with two more denominator and check if those are outside of the nominator range. Then we know the resul: 3 .
Ah, this feels so weird the moment I think about texture interpolation or edge antialias.
With a beam tree we know the slope of our edges. So we should use our compute power for this instead of mislead pixel perfection.
The border pixels of a span in JRSIC need to be calculated by JRISC for aligned access anyway.
It hurts a bit that I would need to pull the information about the neighbour.
I may need to render a whole tree into a pixel with lots of crossings.
32 bit complete solves "scrolling" through a dungeon. I can live with EyeOfTheBeholder like dungeons at the start. Level can be 256 times larger than viewing distance.
Realistically, 32 bit fixed point with antialias and interpolation will look best. Antialias needs a smaller buffer, and is isotropic, in contrast to anisotropic pixels ( which the Jaguar can do).
Smaller framebuffer is need because the blitter has a low fill rate. 
Maybe it is as fast to edge antialias textures? Or range covers two or four texels, then blend.
So when precision of 16bit gets to its limit, the texture does not (only) jump, but blurrs.
Of course, it may make sense to base the blurr on the projection. This would work for small affine triangles. This again is an argument to feed texture projection through the vertices. Two code paths?
When I split up a large polygon for correction, the fragments are still affine and done by the blitter. So they have one blurr. So the blitter does the non-blur 16x16 bi-linear blocks, and JRISC does the edge filling triangles?
That sure will look ugly. I'd say, blurr fades out from 1:1 zoom to 1:2 zoom. Ah, feels like, I just have to try this. I don't want to show my edge phrase trick.
The blitter interpolates without perfect precision and without blurr. I guess, I should care less. Why waste perfect endpoints on a slope calculation which is rounded to fixed point?
Texture mapping runs in pixel mode. We rather want it to run long spans. Edge phrased by JRISC are just too slow. JRISC is only for the top and bottom corner, where the set-up time ob the blitter would be as bad. So we really have vertex-phrases.
Large polygons need to interleave a lot of calculations into their 16x16 bi-linear blocks. The edge triangles and the vertex phrases their vertices. 
Zoommed in textures need fill rate, and we let the blitter do that, while we concentrate on the edges. So in SRAM a buffer of edge phrases to fire, once the blitter has done another bulk renderer.


Explanation on the web needs typescript like the CPU-simulator.
Fun demos can run on the Jag which has limited precision for factors. So we can show off lazy added precision.
Similarly, 8 bit CPUs could try to calculate large runs on 8 bit.
Both CPUs have relative fast branches and memory access.

After trying to understand the 3d pipeline, I could not find a way to make following stuff work:
1) On a texture not leak outside the source region.
2) clipping so that the order of edges is naturally unchanged
3) consistent beam tree; even if I promote scanlines and pixel borders I still need 2) 
4) If I use a BSP to split and order polygons into fragments, I also need precision.
	Sometimes geometry is just so suited for this. Or I start with overlapping boxes, shrink, and half the faces end up on real geometry.
	Does anybody know why Doom has no fragments on inner nodes? I thought we pick polygons as splitters..

Graphics pipeline

Already without any tree the level is a mesh: Vertices, connected by edges, and faces which in turn have a texture.
Lightning on the target hardware can only be baked in or rather simple: No shadows, no use of precision math.
Edges are important for clipping against the viewport and in beam tree.
Date sits in a large flow graph and only local dependend stuff is pushed through it. No large vertex buffer!



Camera has 32 bit position and a transformation matrix with 16 bit entries ( allows typical field of views, but maybe no telescope):
The transition to parallel perspective needs to be implemented differently ( next iteration ).
To reduce the number of exceptions, I may want to use floating point with one exponent per vector. Components have no norm. Only 4 bit exponent.

Clipping per face of frustum. Near plane is at 0 : Don't render planes which cut the camera. 32 bit precision of position
in a large level still may hit this. Also people expect this in the code.
A blanket far plane makes no sense on the target platforms because they are so bad at skyboxes that we can rather use real geometry.

Field of view ( z-buffer scale) and screen aspect ratio (in pixels) make it so that the frustum planes are defined by a vector
with number of pixels * 2^n ( floating point: we always shift around bits. Only real division is expensive).
So clipping a vetex is not a simple comparison, but one side is multiplied by 3 (4) ( oldschool target platform has 3:4 AR).
So ah, 4 is 2^n. 3 is also onyl two bits. Now z scale is more free for me because z-buffer has only limited use on the target platforms.

A cut throuhg an edge results in a fraction ( nominator / denominator ). For perspecitve we divide by z and then one co-ordinate is exactly on the border of the screen,
but both 1/z and other/z have it and there is a sum somwhere and it stays.
Vertices are sorted by Y. In fact we multiply the denominator with the line number. Costs 8 bits. Will mostly suffice.
For triangel setup we need to know the slope of the edge: Another division.
So if the slope is not conclusive, we can always test 2 pixel instead of just one.
Also the target platforms like fixed points / floats. So we have a slope with a fraction and only check a pixel if we come close to it.
Similarly, for the slope of the texture, I can trace a ray to check which of the two or four texels I hit.

Now for a beam tree we need to cross edges to get a point. We even need to do it with 3 edges to decide if one edge needs to be split.
So, lots of divisions and multiplications. One could say that a beam tree really needs floating point.
It was discussed when Quake was new. Quake uses the floats. But it also seems that variable precision is most badly needed here.

To pull in more precision in a chain of MAC, the graph is followed backwards ( each edge has a direction but the back direction is also stored in RAM )
Each node can append more digits for the fraction. We need to know the error of our floats after division. Like:
slope of edge, slope of texture coordinates. We divide U and V by z at the vertex and calculate a slope.
Going along the slope we add those values up, but that is equivalent to multiplication.
We cannot add precision to this code path ( with floats and += ). Exception goes over to MAC and final division.
I don't see no easier way to calculate the uncertainty than by variing the nominator and denominator into their extreme values.
( +1 , -1  or 0, +1 ).

So texture mapping does not interfere with the beam tree, but I want to use at atlas and never have "general protection fault".
That Jaguar blitter can do linear spans. So how do I know that the line hits the correct pixels?
I cannot really. I can only take start and end pixel and the 3 edges in texture space and check for safety distanc > rounding errors due to writing to hardware registers of blitter ( inner loop in C64 ).
Bilinear makes even more sense because I need to check distance in 2d anyway. So it also works for affine. This would all work with floats and some branches .. so beam tree is really expensive? In terms of code size?

About that perspective correction: We divide per point ( span start, or upper left corner of block). So we can keep nominator and denominator up to that point. We can pull in more precision for both.
Only the subspan is does rounding. We say that it is hybrid between pixelation and real vector. We create no pixel gaps. First and last px are precise anyway ( integer ), only the gap is rounded.
When we round in UV space, we only need the precsion there. Grazing incidence on screen is no problem. Only slithers in an atlas woudl be a problem.

High precision clipping ( at screen border or occluders ) gives us high precsion U,V -- read UV within source domain.

Fractions for slope avoid edge crossing on rasterization just as with bresenham. So there is no new branch. Indeed with fractions, the exception branch is only taken seldomly and would perform on branchprediction hardware
like ARM or pentium.. .

BSP merging is a complicate algorithm which would generate a datadependency graph -- not use a given one. An exception in the clipping in the texture may very well reach through the whole Beam tree to up the precision.
Do I persist the whole graph with all pointers into (local) RAM, or do I regenerate it only in case?
The latter is the spirit of the exception. The code path needs to know where to concentrate on.
We have a tree .. which is most of our datadependency graph .. it even includes top level screen borders. 
No matter what the heuristics looked at, we just go up to the parents and get the clipping edges.
Each node could contain a sorted list of vertices of its convex shape2D. Edge definiton may be independent of vertices if that dependency chain is shorter that way.
Sorting information may only be acurate relative to scanlines. Or not even available? What if that subTree is hidden anyway?

Bounding volumes are meshes like everything else. So the beamtree really splits stuff up. Heuristics may want to use horizontal lines for a frame. Now I feel, like MVP should not already do this.

Usually shaders are expensive to load, but either way: Draw convex leaf of BeamTree or draw polygon with BSP as clip mask / scissor mask / stencel mask ( latter ones are usually stored as bitmaps).

Caching of transformed vertices is possible ( read: We don't need to have a transformed buffer as big as the original buffer ) because we use a bounding mesh hierarchy.
Those meshes can overlap and edges can reach into vertices in the next node. So the "current node" where we cache  is really a tree growing in the global tree.
Also this tree as links to from the beam tree. Already without the precision thing we would cache those.


It seems that lazy precision ( precision on demand ) needs BSP for 3d and bottom up beam tree. All vertices and documented beam tree decisions need to exists in memory in order to pull in more precision.
Also the camera position is pulled in lazily. So small movements don't trigger much changes .. if two BSP subtrees are close to each other and the camera far away.
The beam tree gives us convex polygons as leafs. Edges and vertices can have their own data. Because we don't round, they match. It just makes the nominator and denominator smaller.
Clipping is applied like in floating point .. only that the interface is now implemented by the infinite precision fraction class ( still wonder if I may need infinite precision floats ).

While the beam tree is constructed bottom up, of course child nodes are detected top down when their bonding box leaks. Likewise view frustum culling exposes new nodes. Invisible nodes are garbage.
I may need to look up this cache agnostic algorithms, but basically if the garbage is not in the way of defragmentation, I want to keep it as long as possible. The target systems are single threaded 
(okay, the Jag has two CPUs .. but assume that we have music) . Now a reappearing object may need deep inspection and a lot of repairs .. maybe decide based on relative camera movement.

So I don't see much way to back paddel to single frame images. Even the camera rotation, projection on scanlines, and Bresenham may want to pull in more digits.
I could back grow some tricks. Like when all leafs are sufficient convex, just round for the camera. But I don't see how this test is cheaper.

Infinite precision and/or floats need more cycles on JRISC. I estimate twice as much because JRSIC is already so cumbersome to use on integer math.
To make this viable, the precsion part must not dominate the rendering loop. Besically, I need to render simple scenes.
I guess, for edges I should stick with rounded values. 32/32 bit fractions should be good enough for anyone. In debug mode: Count the ambigious pixels.
Likewise texture mapping runs on rounded values. Those are based on floats. So the nominator and or denominator uses all availabe bits (32).
I use the address generator registers to point into a texture atlas. It accepts fraction values in different registers (interleaved x and y).
So I need some shifting after I fetch the fixed point result from DIV. I wonder if DIV is saturated. I may try to DIV the remainder to get the fractions,
but then again I onyl expect like 4 bits for the integer part of the delta registers and 6 or 8 bits for the integer base.

My examples: "Fight for Life" and 3d cars on an "Outrun" track don't need textures with GL_REPEAT. So small integer parts are okay.