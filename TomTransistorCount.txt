Tom has very large regions filled with flip-flops to store information.
There alse seem to be a lot of full adders.
And there are busses and multiplexers.

Flip flop in CMOS constist of two NAND gates.
To read it out a row on the memory bus, you need a gate :
Inverter whose "power line" is off most of the time ( rails inverted ).

I just want to stress that with this 2d geometry with SRAM just like with DRAM
you read out whole rows. The bus is wide.
Also flip flips have a separate input.
Now you need to gate the input to select a row anyway.
So you can select a row for read or for write ( with DRAM it is the same .. capacitor only has one port )
So can, but you need not, to connect the data lines to in put and output.

The hardware multiplier consists of a 2d array of full adders.
The blitter and OP need many adds at once.
Both don't listen for the carry flag so much. Everything can be ripple carry
and skewed over the 4 equal length phases.

x and y are combed into a single address ( likewise u and v ).
But the lower bits of both become the address in the page.
But the memory controller needs to know the address of the page,
to check if it changed. But even here the compare can ripple in the correct direction.

Fortunately the external memory runs at half the speed than the internal transistors.
With 4 phases a register / latch can be used as a pipeline stage.
With external memory being slower by 1 cylce, we can deskew a whole cycle.
Address of page and Address in page are independent anyway.

What I want to say is: Transistor count is easy and layout also.


Data from external memory pins is routed to any of the bytes of the internal bus. The internal bus bytes have "don't care bits" so that the data is not CAS to external DRAM.


to multiplexers become address generators.

The idea with the 64 bit data bus is that the majority of pins and metal lines on the die
carry actual data and not addresses.
So it feels natural to seek synergy and logically expand the 64 bit bus onto the die.
But on the other hand, internal communication can have twice the clock rate and
prefers skewed data.

With so many lines already present and similar to 68k it makes sense have a lot of lines
to declare the bus master.
Back in the day SIMD was not really a thing beside bitwise operation.
With standard definition video ( and CD audio, stereo only) and some buffering, we can overdraw each pixel 4 times
( at the 27 MHz of the Jag ). Sure there should be a hierarchy: Most power for pixel, less for lines, even less for vertices.
We would be lucks if every component on the die could use the bus every 4th cycle to load and store
16 bit pixel values, or instructions, or z-values.
32 bit fixed point or pre-gamma RGBor RGBA or Pointers would be the larger data items: every second cycle.
So in order not to come up with ultra fast pririty circtuitry,
components get assigned slots and signal in advance if they need them and then give them a friend.

So our good old friend, the fast-page mode, does not like those alternating access, but want bursts.
Thus all components hook onto the back door of a part of the internal SRAM.
The bus can be used to copy from and to internal SRAM.
Internal and external address bus point to different addresses.
Transfer can be a range of addresses, but sometimes components write in a somewhat random manner.
So in their local RAM they need to store (address|value) paris.
When their buffer is full, they signal "pressure" and release all writes (and get their reads filled).

Either there is volatile or a flush command, all components need to check each other for address collisions.
We wanted to save on address line :-(

The real Jag indeed has some more busses of differnt width. So we are not yet at the full transistor count.
Now we wonder: How does read after write and write after write consistency work with Intel CPUs ?
Internal eventual consistency? CPU and video/audio out are extern, but DRAM, SRAM, blitter. OP, and GPU are intern.
Every buffered read and write gets a timestamp.
Basically we have to sort all requests by address to check for collsions.
We do this on the fly. So we may be late the detection.
Then transfer from  DRAM to and from the buffers need to be repeated in correct order.
Processor operation needs to be rolled back .. which would mean a shadow set of registers
or two: One freshly created and an old one where everything is already consistent.

Adresses need further sorting to detect shared pages and shared phrases. Also a page break lets the scheduler/router reevaluate the pressure.

A problem may be the external ROM, DSP and CPU which directly access some data lines for the RAM directly,
and some indirectly through the bus interface.
Usually those other components are told to get off the bus.
It is not quite clear why TOM should activate the memory chips for every CPU read.
It could deliver cached values, like it already has to deliver the other bits on the databus.




Unified OP, Blitter, and GPU.
All those have register ( a lot of ), counters, adders, and Load and Store. CMP, AND, OR.
GPU as mul and div and ScoreBoard for register reads ( MAR is a register , flags are in a registers )
GPU and OP can branch .. though modern blitter pixel shaders also branch.

So they all have a ALU with two inputs and two outputs. We already discussed that SRAM naturally only has one output.
So there is a multiplexer as a proxy and the ALU sits like the spider in a corner of 4 SRAM areas.
Also the value of the last instruction is at the proxy ( like it is in the Jag), but it is available right in the next turn?
Unskew for flags costs a cycle.
The pipeline in the GPU is kinda interesting and one could try to run two threads .. like now, but with different control flow. Oh that does not really help

The blitter basically has a 64 bit ALU with SIMD. It needs to write back the counters. The blitter has a very regular access pattern and thus doesn not need a scoreboard. Also it can use indexed addressing on the register file to save the write back.
GPU access on blitter goes through index offset logic.

They all seem to need 64 bit registers and this command (instead of complex bus logic):
add 0,1 , with detect carry after bit #3 
J ~C
{
	load ( phrase aligned: 1 ), 2
	// or store
}
move 2( word selector), 3     ;  ROR AND MOVE

Or better use 128 register > than memory . We need to store dirty flags. And maybe store addresses rigth there for transparency in code?
I like the DIV command in JRISC, though it basically cries for floating point. One could shift after the fact.
I think I like float more than fixed point. 16x16=>32 multiplication is also nuts compared to float.
32bit floats are easy to use. I don't like floating point add. Larger register with not normalized float.
Exponent makes every add a Shift like on ARM . So is it okay? JRISC already has overflow bits in MAC: 

Sub exponents ; can be hidden by skew
shift
add
Carry? shift in         ; skew problem
:  shift out the zeros.

MAC is a 64 bit register which stores bits + carries for the pipeline. Also there is a 32 bit register for the pushed out bits.

I did some thinking about enventual consistency. We could unify the register bus with the memory bus. We allow all memory operations .. even Load into register file. But scoreboard / dirty flag
stops loading or speculative: Stops write back from ALU. It could even be possible to write into the output buffer for video?
A time range of 16 clicks is allowed to resolve stuff.
Do we need it? There is a feedback between GPU and Blitter. There could be other constructs .. I dunno
Now I feel like it would be faster to just have a fabric and every cycle compare all addresses with all others.
Eventual consistency makes no sense on a chip .. with max 16  .. ah what do I say . .yeah maybe if I open up the details in the OP?

Still interesting. Even if I compare all adresses (21 bit / 32 bit maybe) generated by all components every cycle.
So the compare matrix is not the problem, but I need a star shaped ( hub ) network of address lines and many metal layers in the fab?
Anyway, I would like to assemble read and writes to memory .. ah do it in the memory controller.
This is what a cache is for. Memory is supposed to be congested and thus write back values are made to wait an may gather some more writes.

For the application -- this game console -- it would proably be enough to apply locks on memory ranges in every memory.
The code becomes a bit ugly, but you find errors early. So when we use the blitter to load code, we guard the switch to execution of code by means of
changed address range ownership. Or basically all internally memories should be double or quad buffers.
For example one buffer is code, the other is data, one receives new code via blitter, once receives vertex data via blitter.
Or one is  code which is modified right now .
Ownership communication is: You take ownership at access. You can send release command on extra 8 bit bus ( identifies block with is now free).
This and priority for main bus is round robin. Ah no. Blitter get texture and GPU load need the bus ASAP. So they have additional request lines for urgent loads ( not for store )

Due to the rust like owner ship, a component can cache values form other components without consistency checks.

So I don't really know what a half written color LUT should mean.
Only thing I understan would be to race the beam and swap out quartes of the palette for different sprites.
Those guard checks cost some time and the code gets complicated. But I need locks and syncs anyway. For example the blitter regs.
There the time of the CPU store is of no importance, but the blitter writes prevail.

QuadBuffer memory would fit to the quad register file.
The linebuffer is a double buffer in the Jag.
A quad buffer could be used for low res: multiple lines and for hi-res: no splitting of sprites on scanline needed.
ScoreBoard for writes is bad because I cannot queue those.



About the OP

Another indirection where we store the spans for each scanline.
Lets assume we want to fit it into two object phrases.
Those we keep:
* These twenty-one bits define bits 3 to 23 of the data address
* 12 bit xpos
* FirstPix within phrase

0-7 Hscale  becomes the U in the texture mapping UV
8-15 Vscale becomes the V in the texture mapping UV
Remainder not only for U,

but for V.

Replace Index with a color offset.
Add color increment.

Is space left for z and delta_z ? We may compress data a bit by sorting by z and using relfect to draw back to front so that we always use the z-start value (and not something in the center which would be unfriendly to OP)


JRISC is MIPS ( 32x32 register file) where one source is also the target to have 16 bit encoding. This is similar to many onthers like Atmel8bit, SuperFx, SH2.
I wonder if reduced instructions can be sorted in a different way than MIPS does it ( based on immemdiate). That is more 32bit encoding stuff like also for RISC_V.

Due to pipeline, the source registers are encoded as address. One bit denotes a literal => 2*6 = 12 bit
We could specify a target:
writes to second Source?  1 bit
	ALU
		commute
			4   2 bit
		sub
			only instruction which allows a constant in both operands (xor)
	LoadStore
		Move  pointer          <- regs only 
		Add   Adreessing mode  <- literal   .. implicit register like in 6502 (X,Y), 68k, z80 (SI, DI)
flags?  Write to Flags? 1 bit
	writes to
		CMP
		Test
		Btest
	reads
		Jump  ( all jumps are conditinal .. all other instructions are not)
	both
		ADC
		SBC
implicit
	Jump