So it seems like there is no hybrid between scanline rendering and painters. At least I get scary worst case (memory scenario).
Now I am almost sure that I don't have enough code memory to squeeze in scanline rendering (16 bit math, 32 bit math) and texture mapping (texture coordinates at corner, sub-spans, register address)
at the same time. Maybe I even want shading because Fight4Life looks so flat.
I can define a happy path. Polygons with max vertices. Like obviously we need at least 3. 4 happens due do clipping (on frustum) or occlusion. 5 can also happen.
Convex. The rasterize needs to deal with non-convex polygons. Ah wait, that is ugly along the edges. I can only store two edges in the registers.
So, mostly convex. And where was I : Indices to at least 5 vertices .. make that 8.

So it can happen that we cannot write out the complete polygon. To make the sub-span renderer happy, there needs to be a winner on merge and a default on split.
So the effect is like this: Many small polygons in front of one large ( biped in front of a wall ), fills up my memory with lots of "virtual" polygons to draw "around" the characters.
Lets just say I accept as many (additional) virtual polygons as real ones (256+256) and hence have limit for the memory usage.

Z-sort may be a way to avoid this, but is not without hassle. So obviously I have determined which polygons are in front.
Ah, all the large "background" polygons need to be resolved within. Likewise the small polygons should be resovled within their group because most results are easy to process.
The BeamTree is looming in the background because the active edges list needs fast inserts, which needs a tree.
A hybrid solution would be two fold. I check all edges for the next crossing and may skip some lines completely. This is kinda a test from the BeamTree.
Then when the number of edges gets two big for local memory and brute force memcpy, a visble edge is chosen to split left / right.

Polygons may be split onto both sides. So I later may want to fuse them. By using a real edge instead of vertical (from a vertex) or blocks,
I at least safe one polygon from a split. Now this edge may even belong to two poylgons, or it is an edge of an object and occludes stuff behing it : So many cases without polygon splits!
It is still a bit ugly that I want to keep code around to check.

I thought about the span buffer Quake is said to have. It fits the blitter. With a low resolution I might write span starts at pixel positions.
The blitter can clean this buffer. Even if this solves allocation, I still need pointers to jump over other polygons and to advance into the next scanline, heck, even for overdraw.
This becomes a deferred renderer: Draw everything flat shaded with z-buffer. Use blitter (pixel mode) to find span length.
GPU calculates texture coordinates for start and end. Use blitter (pixel mode) to fill span. Interleave this to keep blitter and GPU occupied.
Like for one blitter action the GPU does the left pixel and for the other the gpu does the right pixel.
The backbuffer is for the first part. If I don't mind screen tearing, the second pass renders to the 16bpp front buffer. I hope that Grouraud shading code paths also fits into local memory.
"It normally compares the source data register with the pattern data register," => "The action of the Blitter can be stopped altogether"

After clipping, affine texture mapping does not work anymore. I guess that the KISS method is to clip perfectly, but then interpolate on the edges.
I do not like that I need one extra division per scanline to recalculate the delta. Divisions should be used for perspective correction..
uh but which also needs one delta calcuation per scanline. Triangulation would indeed avoid this. But then again, I have one division free per scanline iteration.
Guard band for very small affine triangles. A nodge towards perspecitve correction for larger. Maybe have an executable with either code path / or a quality option?

Sorting (by y) is so incredible fast that I want to start with scanline rendering for the visibility determination, but defer the rasterizer due to code cache size.
Memory managemnt is like: count number of vertices for each y "bucket". Then fill in the vertex IDs. Bucket sort.
Even packing in internal memory would be possible. load ror add rol (either counter or increment) store.
Quicksort and Mergesort are also good conentenders, but have weird worst cases when I take internal memory constraints into account.
Finally, for cockpit view with like 120 lines on screen, bucket sort just plain wins.

Active edge list can be a b-tree with root in internal memory and leafs in external memory.

Z-resolution pulls in float too often. So we reduce its count. First only for xy overlap (perfect). Then check for z exp, then z mantissa, then cut line projected on screen (32 bit probably).

I determine visible surfaces in a separate pass for profiling. But is a bit difficult to destill the clipping information. I seems like I need to repeat the active edge list stuff in the next pass.

Small triangles use the affine rasterizer. I use the guard band to allow (small) triangles to use this fast pass independently from some rather arbitrary clip. Or do I? Whan about occlusion?
Maybe I should not play tricks. Affine is only for unclipped, unobscured, small area*delta_z triangles.
Anything larger uses edge interpolatation and then span interpolation. So actually, I want to correct perspective on the edge if I have to interpolate the span anyway.
For larger triangles add subspans. I know I loved tiles, but subspans fit in so easy when you look at the code. DIV is as fast as 4px subspans. 4x4 tiles make no sense on this hardware.
So anyways, I want to do the perspecitve correction on real pixel positions -- so not exactly on the edge, but the first and last pixel.
Edge interpolation is not enough. For triangles I can invert a 2x2 matrix. This is also true for trapezoids if there were no rounding errors.
So each edge has to pick a pixel on the oposite side. Actually, I feel like if this is the result of clipping on the frustum, there is an obvious way for to get deltas along x or y or even both.
Then for the other we can save one mul, but still need the DIV. Ah the one common DIV. Anyway: for a trapezoid: pick the longer span ( top or bottom).
Bresenham on the edges allos me to use diffrent deltas for x U V W. One DIV per edge pales in contrast to the 3 DIVs per span that I need.
If there is memory left, small integer division could use a jump table. I need this for 1px long spans. For 2px I can use SBC instead of other instructions. For 4px shift. Both are 32 bit.
So basically only jump (every jump has a condition field) if power of two (2^0). Check: BSCAN, adq, SHL -> zero flag.

Since I only correct visible pixels, I need to do this for all edges of the polygon and those cutting in. Maybe I should select one horizontal delta value.
Just feels like I should go back for infinit precision raytracing maths for this one (ah, 32bit should be enough). Only triangles would short-cut to screen inverse matrix.
Should I just log the active edge list? In pass 1 I need to stop on each line with any change, but per polygon I just log changes within. Changes happen at a pixel.
So from the pixe I go to the left and search for left edges. Then I go to the right and search for right edges. Ah, thanks to clipping I can just choose the shorter side.
Each edge in this list has a link (8 bit ID, max 12 bit because 4096 polygons are beyond Jaguar Caps) to external memory. There is a link to the current position in the log.
Memory management needs a lot of linked lists. But the GPU cannot burst read items anyways. Just squeeze everything into 64 bit (reads).
Writes are queued in internal RAM and copied by the blitter. Changes in the active edge list are real vertices and crossings (some of it with the frustum).

An interesting number to profile would be the time spent on 32bit and float calcs.

Portal rendering. The beam tree leaves are convex polygons .. so portals.
I want to improve on Sky Hammer in the scenes where there is an enemy covering a lot of the screen or when I peek around a corner.
In both cases, portals help. Mostly I don't get the artwork in Skyhammer. All grey and almost no direction. I will need to generate some CRY textures with caching for shading.
When I find the overlap of two convex polygons, using a BSP tree is not the fastest way, although it may result in the smallest code size.
So we check the vertices of one polygon relative to the edges of the others. The order of the edges defines a BSP .. uh, but we don't split edges?
So what is the result: For each other vertex I see if they are inside of these edges. So it is a bit pattern. Does this help me? Some assembly trick? Did not work well last time.
What if I have both views?
Find cuts:  Number of cuts.
0 one inside one other ( all vertices inside all edges . Check from both sides.). Non-overlapping: Early out. Overlapping feels like collsion simulation and it is true. But I only deal with polygons with up to 4 vertices. 16 compares hurt, but I don't want to pull in separating axis theorem or CJK iterations and am not interested in contact points here. Maybe recheck later.
1 one.vertices, cut, other.vertices, cut

Crossing two vertices on the other side.
Does this work for the whole cone ? Check that camera is above polygon surface. Vertices of the polygon surface may be behind the camera. The latter test is the naive way to solve this. But it would need to chain divisions or at least rationals.
homogenous coordinates?
Other.Edge through face (vertices on both sides). Other.Edge . Volume = camera-Other.vertex  x   Other.edge x this edge  .. sign changes.
Early out vs collect signs first (branch free, optimized JRISC can be fast) and divide later. Okay, is probably not fast due to floats, but uh..
So on each polygon we get signs per vertex, and changes per edges. And we get links between edges. We get links because we check each edge with each other edge.
To justify this, in the outer loop we go over the polygon and calculate camera x edge. Then in the inner loop we inner product this with the corner rays from the camera.

The result is again a convex polygon. Calculate z. Sort. Ranges. Slope if deltaZ>2. X Ranges. Affine Slope for delta(round(X))>1.
The remaining space in the portal is not convex. But we now only need to cosider the visible edges and vertices.
The newly drawn polygon gets its central (longest) edge extended. Repeat recusively.
When drawing a triangle stripe/fan, the shared edge is extended. I don't want to pursue any heuristics into the third polygon.

How is the counting if we have a fully-in or fully-out early out? Like two convex polygons are fully inside the screen? I could try to profit from the code: Find the ovrerlap. Check z.
So we would allow up to two convex polygons in a leaf. When then another arrives, the old ones need to be converted into a BSP. Lazy BSP. If one polygon is fully inside leaf, but next is not: BSP!
Do I want 3 polygon inside checks? Maybe the new polygon is occluded? But I only get a probalistic answer. The new polygon may be fully inside the already drawn polygon, but behind.
So I want 3 inside relations? Or rather 6?

To allow a fast implementation of Elite, I should probably use the shiloutte of the ships as rough splitters, and then draw the polygon faces.
The nice thing compared to an indoor level is, that I can just sort by z on screen. Or maybe mix in apararent screen size ( space station, planet). Simple rule, easy to tune.

As a tree this would be kinda like a B-tree? So not binary. Each node divides the convex parent into one or two convex children, plus the rest.
Tree merge needs 4x4 comparisons. I really want this to happen while the blitter is occupied, but I don't see how.
How do I rasterize a half moon? It can have two spans per scanline. So I need to find the splitting vertex.
The floor line merging in Doom is a bit suspicous. I would understand mergin in DukeNukem3d with all the portals, but the Doom BSP has less splits, I think?
Anyway, in a true 6DoF I feel like "half moon" covered polygons happen a lot and the coordinate independent split feels weird. 90% of the speed of a full merged solution.
A complex polygon renderer can run parallel with the blitter. So we want complex leafs. Then we can calculate the next leaf (sort y), while we rasterize the current one.

Setting up the blitter is quite expensive.
So we want to draw as long lines as possible.
People don't complain about squares scrolling through.
Thus switching the directions ( x,y, both diagonales ) is okay.
Optimize to minimize scanlines and for minimal zchange ( hard limit on max z change)
Small triangles are drawn using affine transormation.
Quake and descent had scanline spans.
So when the scanline is really long, divide the rest in 16px or 8px spans.
In between, switch on perspective correction on the edges.
The top and bottom tip can stay affine.

Set up changes the least from line to line for affine mapping with subpixel correction ( we don't recalculate delta per scanline).

Color Ram can be used as texture cache because I don't plan to use palette in my artstyle anyway, but I need all GPU RAM for vertices. Or whatever, anyway,
Skyhammer showed that the small cache is best used when we draw quads.
The blitter can efficiently copy a rectangle from DRAM to color RAM. I mean, yeah: 2 cycle read, 1 cycle turn around,  4 cycles for write. But for a Jag this is fast, especially when we zoom in, or reuse tiles.
Again, the 8x8 quads on the output are beat. Just like on N64 the way to go is to split up a large polygon into a grid of quads. But similar to 3do we rely on this also for perspective correction.
I guess that recalculation of the gradient per scanline is faster than drawing two triangles. Maybe we can draw the top and bottom of the quad as triangles.

Also: Drawing direction can be choosen, for longer runs! And we can use rectangles for longer runs. Regarding the beam tree: Maybe we don't fuse leaf nodes if this busts the cache.
Quads don't dominate our geometry. We can still have Doom Mesas. We load rectangles based on the rays casted from the BeamTree leafs.
For small quads skinned onto a model it makes sense to to store them in a page. Though one page flip per 16px .. just read how many cycles each pixel needs right now. Ah, the blitter eats one cycle per line change and the page flip would eat two more.
This hardly justifies another code path. We don't cache on scale down!
Or at least we would only cache the output. But it does not help much as we have page misses all the time anyway.
The Jag has enough RAM so that we don't need tiles.

Squares are so attractive because they don't need the edge calculation.
The spans can be shorter and the GPU is still faster than the blitter.
Squares only make sense for large triangles.
Spans are as fast because divsion .. even though it inssist on 32 bit, only takes 18 cycles.
The blitter cannot do 4 px in 18 cycles.
Only if I try to do other stuff ( interupts ) while drawing the span, squares / rectangles would make sense.
As we all know there is no 64 bit texture source beside DRAM.
If we go for 50 fps, we could render to linebuffer .. do we go ?

For perspective on the edges ( const-z renderer like Doom ),
do I calculate exactly on the edge? Maybe for this precision thing?
Like I use 1/z and multiply(2 MAC) with the edge vector in texture space.
One division ( fixed point ) for linerp. So per line: 3 div, 4 mul.
2 Mul with fraction to go on the first pixel center.
Pro: Can be proven to stay in triangle. With low z variance we do affine anyway:
We can do multiple lines with one division for width and 2 Mul for subpixel.
Still seems faster than to calculate the diagonal.

Calculate on the first and last pixel: two div, 4 mul. integer div.
Bresenham to jump on pixel center. Adds for all 3 components of homogenous coordinates.
Pro: perspecitve closer to actual pixel. natural transition to affine.
Staying inside texture triangle can be forced by precsion on demand.

So I always calculate first and last pixel on the GPU,
but the blitter already for short spans allows me to write unaliged without RMW.
Maybe it makes sense to write up to 4 pixel into the source register ..
Todo: Test if 2n sourc register is shifted over without a read?
Still with 4 pixel like in Gouraud shading,
it might be interesting to fetch the texture using the GPU.
GPU master is 64 bit. DRAM is 64 bit.
eat dupAlicated phrase access, 
load
write into blitter
phrase mode
Still: A second code path? Lots of GPU cycles on short spans???
At least need to combine a short and a long span for this.
Oh, registers get crowded.
Or should I use the blitter to load the texture into GPU RAM?
Then use GPU to fill the phrase at the end of the line and blitter pixel mode for the 8 px spans.
No. Avoid all the storage.
Copy target has 2^n width.
GPU:
	2 add, 2 adc, move, shift, or, load, add light (saturate?unpack or high word + carry)   8 (4)
	(fraction."sign" xor destination.bit0) and shif 16 => shift_px and or  5
	saturated light is what makes the blitter the winner.
Gouraud is similar: needs <=4 GPU pixel. If saturation? Be sure to start on non-saturated side or from the middle.
	

To avoid divisions for short spans I thought about a lookup table.
Span length, subspan or 1/x lookup and MUL. MUL is faster of course. Oh my. Also less code.
Edges are expensive. So hide some of the calculation in a span?

If packing on GPU would not be so slow,
Then even the UV need the 16 bit shift.

n-gons help with long spans. Like Descent: If same skin and some tolerances (z, same face) are met.
I could even do scanline render for concave.
